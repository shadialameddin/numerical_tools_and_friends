{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["CUDA_VISIBLE_DEVICES=0,1 python use_gpu.py"]}, {"cell_type": "markdown", "metadata": {}, "source": [" GPU activities [CUDA memcpy HtoD] [CUDA memcpy DtoH]<br>\n", "nvprof python use_gpu.py"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [" # https://www.tensorflow.org/guide/gpu\n", "# from __future__ import absolute_import, division, print_function, unicode_literals\n", "import tensorflow as tf"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tf.debugging.set_log_device_placement(True)\n", "gpus = tf.config.experimental.list_physical_devices('GPU')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if gpus:\n", "    try:\n", "        for gpu in gpus:\n", "            # Memory growth has to be enabled in order to not block the whole GPU memory\n", "            # Currently, memory growth needs to be the same across GPUs\n", "            tf.config.experimental.set_memory_growth(gpu, True)\n\n", "            # Restrict TensorFlow to only use the first GPU\n", "            # tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n\n", "            # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n", "            # tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n\n", "            # Create 2 virtual GPUs with 800MB memory each\n", "            # tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=800),tf.config.experimental.VirtualDeviceConfiguration(memory_limit=800)])\n", "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n", "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n", "    except RuntimeError as e:\n", "        # Visible devices must be set before GPUs have been initialized\n", "        print(e)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for i in range(50000):\n", "    # Create some tensors\n", "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n", "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n", "    c = tf.matmul(a, b)\n", "    d = np.array(c) # device to host\n", "    # print(c)\n\n", "    # Place tensors on the CPU\n", "    with tf.device('/CPU:0'):\n", "      a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n", "      b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n", "      c = tf.matmul(a, b)\n", "    d = np.array(c)\n", "    # print(c)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Not tested, how to use multiple GPUs<br>\n", "NUM_GPUS = 2<br>\n", "strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=NUM_GPUS)<br>\n", "config = tf.estimator.RunConfig(train_distribute=strategy)<br>\n", "estimator = tf.keras.estimator.model_to_estimator(model,config=config)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["tensorboard profiler??"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}