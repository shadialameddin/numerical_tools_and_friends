{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Numba for the GPU\n",
    "[CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html)\n",
    "\n",
    "Note: pyCUDA which requires writing C code in python is not tested here\n",
    "\n",
    "## Universal and generalise functions ufuncs/gufuncs\n",
    "\"ufuncs\" operate in an __elementwise__ fashion. Hence, they are suitable for parallelisation.\n",
    "Numba finds the broadcast rules for a defined scalar function of all the inputs.\n",
    "\n",
    "Ufuncs that use functions (`exp`, `sin`, `cos`, etc) on large data sets run well on the GPU. `np` math functions won't work on the device but their `math` counterparts do."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import vectorize\n",
    "from numba import cuda\n",
    "from numba import guvectorize\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a+b:\n",
      " [3]\n",
      "a+b:\n",
      " [4]\n"
     ]
    }
   ],
   "source": [
    "# an explicit type signature has to be defined, use float32 when possible for faster runtime\n",
    "@vectorize(['int64(int64, int64)'], target='cuda')\n",
    "def add_func(x, y):\n",
    "    return x + y\n",
    "\n",
    "print('a+b:\\n', add_func(1, 2))\n",
    "print('a+b:\\n', add_func(2.3, 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Device functions\n",
    " They may be called only from a GPU one.\n",
    "\n",
    " CUDA compiler inlines device functions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "array([10.107338], dtype=float32)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@cuda.jit(device=True)\n",
    "def device_exp(a):\n",
    "    return math.exp(a)\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='cuda')\n",
    "def function_to_be_compiled(a,b):\n",
    "    return device_exp(a) + device_exp(b)\n",
    "\n",
    "function_to_be_compiled(1,2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GPU memory\n",
    "It is good to allocate device memory once and refilling it with host data in runtime"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.07 ms ± 115 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "3.8 ms ± 144 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "1.59 ms ± 22.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.65 ms ± 42.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "725 µs ± 41.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@vectorize(['float32(float32, float32)'], target='cuda')\n",
    "def add_func(x, y):\n",
    "    return x + y\n",
    "\n",
    "n = 1000000\n",
    "x = np.arange(n).astype(np.float32)\n",
    "y = 2 * x\n",
    "%timeit x_device = cuda.to_device(x)\n",
    "x_device = cuda.to_device(x)\n",
    "y_device = cuda.to_device(y)\n",
    "\n",
    "%timeit add_func(x, y)\n",
    "%timeit add_func(x_device, y_device) # the output is still a numba.cuda.cudadrv.devicearray.DeviceNDArray\n",
    "\n",
    "out_device = cuda.device_array(shape=(n,), dtype=np.float32)  # does not initialize the contents, like np.empty()\n",
    "%timeit add_func(x_device, y_device, out=out_device)\n",
    "%timeit out_host = out_device.copy_to_host()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generalised functions `gufuncs`\n",
    "\n",
    "Generalized ufuncs (that need to broadcast one of it's inputs) need a signature that shows the index ordering when dealing with multiple inputs. The last argument of a `gufuncs` is their output array."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# have to include the output array in the type signature, '(n),()->(n)' maps a 1D array and a scalar to 1D output\n",
    "@guvectorize(['(float32[:],float32, float32[:])'], '(n),()->(n)', target='cuda')\n",
    "def cuda_add(x,y, out):\n",
    "    for i in range(x.shape[0]):\n",
    "        out[i] = x[i] + y\n",
    "\n",
    "cuda_add(np.ones(3),1.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "array([2., 2., 2.], dtype=float32)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "argv": [
    "/home/alameddin/z_nosync_packages/pyenv/versions/3.6.9/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}