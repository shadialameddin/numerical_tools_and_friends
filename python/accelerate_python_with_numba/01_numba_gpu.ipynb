{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Numba for the GPU\n",
    "## 1- Universal functions (ufuncs)/gufuncs\n",
    "\"ufuncs\" operate in an __elementwise__ fashion. Hence, they are suitable for parallelisation.\n",
    "\n",
    "Numba will figure out the broadcast rules for a defined scalar function of all the inputs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "CudaSupportError",
     "evalue": "Error at driver init: \n[100] Call to cuInit results in CUDA_ERROR_NO_DEVICE:",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mCudaAPIError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m~/z_nosync_packages/pyenv/versions/3.6.9/lib/python3.6/site-packages/numba/cuda/cudadrv/driver.py\u001B[0m in \u001B[0;36minitialize\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    230\u001B[0m             \u001B[0m_logger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'init'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 231\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuInit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    232\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mCudaAPIError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/z_nosync_packages/pyenv/versions/3.6.9/lib/python3.6/site-packages/numba/cuda/cudadrv/driver.py\u001B[0m in \u001B[0;36msafe_cuda_api_call\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m    293\u001B[0m             \u001B[0mretcode\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlibfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 294\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretcode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    295\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0msafe_cuda_api_call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/z_nosync_packages/pyenv/versions/3.6.9/lib/python3.6/site-packages/numba/cuda/cudadrv/driver.py\u001B[0m in \u001B[0;36m_check_error\u001B[0;34m(self, fname, retcode)\u001B[0m\n\u001B[1;32m    328\u001B[0m                     \u001B[0;32mraise\u001B[0m \u001B[0mCudaDriverError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"CUDA initialized before forking\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 329\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mCudaAPIError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mretcode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    330\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mCudaAPIError\u001B[0m: [100] Call to cuInit results in CUDA_ERROR_NO_DEVICE",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mCudaSupportError\u001B[0m                          Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-30-4af99ea87359>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'a+b:\\n'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madd_ufunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/z_nosync_packages/pyenv/versions/3.6.9/lib/python3.6/site-packages/numba/cuda/dispatcher.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kws)\u001B[0m\n\u001B[1;32m     86\u001B[0m                       \u001B[0mthe\u001B[0m \u001B[0minput\u001B[0m \u001B[0marguments\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m         \"\"\"\n\u001B[0;32m---> 88\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mCUDAUFuncMechanism\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkws\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/z_nosync_packages/pyenv/versions/3.6.9/lib/python3.6/site-packages/numba/npyufunc/deviceufunc.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(cls, typemap, args, kws)\u001B[0m\n\u001B[1;32m    292\u001B[0m                 \u001B[0many_device\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    293\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 294\u001B[0;31m                 \u001B[0mdev_a\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_device\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstream\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    295\u001B[0m                 \u001B[0mdevarys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdev_a\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    296\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/z_nosync_packages/pyenv/versions/3.6.9/lib/python3.6/site-packages/numba/cuda/dispatcher.py\u001B[0m in \u001B[0;36mto_device\u001B[0;34m(self, hostary, stream)\u001B[0m\n\u001B[1;32m    206\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    207\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mto_device\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhostary\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 208\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mcuda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_device\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhostary\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstream\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    209\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    210\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mto_host\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevary\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/z_nosync_packages/pyenv/versions/3.6.9/lib/python3.6/site-packages/numba/cuda/cudadrv/devices.py\u001B[0m in \u001B[0;36m_require_cuda_context\u001B[0;34m(*args, **kws)\u001B[0m\n\u001B[1;32m    222\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mfunctools\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwraps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    223\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_require_cuda_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkws\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 224\u001B[0;31m         \u001B[0;32mwith\u001B[0m \u001B[0m_runtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    225\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkws\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/z_nosync_packages/pyenv/versions/3.6.9/lib/python3.6/contextlib.py\u001B[0m in \u001B[0;36m__enter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     79\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__enter__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     80\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 81\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgen\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     82\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     83\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"generator didn't yield\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/z_nosync_packages/pyenv/versions/3.6.9/lib/python3.6/site-packages/numba/cuda/cudadrv/devices.py\u001B[0m in \u001B[0;36mensure_context\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    120\u001B[0m         \u001B[0many\u001B[0m \u001B[0mtop\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mlevel\u001B[0m \u001B[0mNumba\u001B[0m \u001B[0mCUDA\u001B[0m \u001B[0mAPI\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    121\u001B[0m         \"\"\"\n\u001B[0;32m--> 122\u001B[0;31m         \u001B[0;32mwith\u001B[0m \u001B[0mdriver\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_active_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    123\u001B[0m             \u001B[0moldctx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_attached_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    124\u001B[0m             \u001B[0mnewctx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_or_create_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/z_nosync_packages/pyenv/versions/3.6.9/lib/python3.6/site-packages/numba/cuda/cudadrv/driver.py\u001B[0m in \u001B[0;36m__enter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    385\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    386\u001B[0m             \u001B[0mhctx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdrvapi\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcu_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 387\u001B[0;31m             \u001B[0mdriver\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuCtxGetCurrent\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbyref\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhctx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    388\u001B[0m             \u001B[0mhctx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhctx\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mhctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalue\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    389\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/z_nosync_packages/pyenv/versions/3.6.9/lib/python3.6/site-packages/numba/cuda/cudadrv/driver.py\u001B[0m in \u001B[0;36m__getattr__\u001B[0;34m(self, fname)\u001B[0m\n\u001B[1;32m    272\u001B[0m         \u001B[0;31m# Initialize driver\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    273\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_initialized\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 274\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minitialize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    275\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    276\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minitialization_error\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/z_nosync_packages/pyenv/versions/3.6.9/lib/python3.6/site-packages/numba/cuda/cudadrv/driver.py\u001B[0m in \u001B[0;36minitialize\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    232\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mCudaAPIError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    233\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minitialization_error\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 234\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mCudaSupportError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Error at driver init: \\n%s:\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    235\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    236\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpid\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_getpid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mCudaSupportError\u001B[0m: Error at driver init: \n[100] Call to cuInit results in CUDA_ERROR_NO_DEVICE:"
     ]
    }
   ],
   "source": [
    "from numba import vectorize\n",
    "\n",
    "# to use CUDA, an explicit type signature has to be defined\n",
    "# USE float32 when possible for faster runtime\n",
    "@vectorize(['int64(int64, int64)'], target='cuda')\n",
    "def add_func(x, y):\n",
    "    return x + y\n",
    "\n",
    "print('a+b:\\n', add_func(1, 2))\n",
    "print('a+b:\\n', add_func(1.3, 2))\n",
    "print('a+b:\\n', add_func(np.ones(3), 2))\n",
    "\n",
    "# Ufuncs that use special functions (`exp`, `sin`, `cos`, etc) on large data sets run especially well on the GPU.\n",
    "\n",
    "# Device functions may be called only from a GPU one\n",
    "# CUDA compiler inlines device functions\n",
    "from numba import cuda\n",
    "# * `np` math functions won't work on the device, you need to use their `math` counterparts instead.\n",
    "import math\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def device_exp(a):\n",
    "    return math.exp(a)\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='cuda')\n",
    "def function_to_be_compiled(a,b):\n",
    "    return device_exp(a) + device_exp(b)\n",
    "\n",
    "### GPU memory\n",
    "# It is good to allocate device memory once and refilling it with host data in runtime\n",
    "from numba import cuda\n",
    "n = 100000\n",
    "x = np.arange(n).astype(np.float32)\n",
    "y = 2 * x\n",
    "x_device = cuda.to_device(x)\n",
    "y_device = cuda.to_device(y)\n",
    "\n",
    "print(x_device)\n",
    "print(x_device.shape)\n",
    "print(x_device.dtype)\n",
    "\n",
    "%timeit add_ufunc(x, y)  # performance with host arrays\n",
    "%timeit add_ufunc(x_device, y_device)\n",
    "\n",
    "out_device = cuda.device_array(shape=(n,), dtype=np.float32)  # does not initialize the contents, like np.empty()\n",
    "%timeit add_ufunc(x_device, y_device, out=out_device)\n",
    "out_host = out_device.copy_to_host()\n",
    "print(out_host[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ufuncs broadcast a scalar function over array inputs but what if you want to broadcast a lower dimensional array function over a higher dimensional array? This is called a generalized ufunc (\"gufunc\"), and it opens up a whole new frontier for applying ufuncs.\n",
    "\n",
    "Generalized ufuncs are a little more tricky because they need a signature (not to be confused with the Numba type signature) that shows the index ordering when dealing with multiple inputs. Fully explaining \"gufunc\" signatures is beyond the scope of this tutorial, but you can learn more from:\n",
    "\n",
    "Let's write our own normalization function. This will take an array input and compute the L2 norm along the last dimension. Generalized ufuncs take their output array as the last argument, rather than returning a value. If the output is a scalar, then we will still receive an array that is one dimension less than the array input. For example, computing the row sums of an array will return a 1 dimensional array for 2D array input, or 2D array for 3D array input."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from numba import guvectorize\n",
    "import math\n",
    "\n",
    "@guvectorize(['(float32[:], float32[:])'], # have to include the output array in the type signature\n",
    "             '(i)->()',                 # map a 1D array to a scalar output\n",
    "             target='cuda')\n",
    "def l2_norm(vec, out):\n",
    "    acc = 0.0\n",
    "    for value in vec:\n",
    "        acc += value**2\n",
    "    out[0] = math.sqrt(acc)\n",
    "\n",
    "angles = np.random.uniform(-np.pi, np.pi, 10)\n",
    "coords = np.stack([np.cos(angles), np.sin(angles)], axis=1)\n",
    "print(coords)\n",
    "\n",
    "l2_norm(coords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2- CUDA Python kernels\n",
    "\n",
    "Custom CUDA Kernels in Python with Numba\n",
    "Multidimensional Grids and Shared Memory for CUDA Python with Numba\n",
    "\n",
    "# pyCUDA"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "argv": [
    "/home/alameddin/z_nosync_packages/pyenv/versions/3.6.9/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}