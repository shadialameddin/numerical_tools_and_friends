{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CUDA Kernels\n",
    "\n",
    "CUDA kernels are arbitrary functions (not just element-wise) that run on the GPU in parallel by different CUDA threads.\n",
    "\n",
    "- __Execution configuration__ of a kernel controls the grid where the kernel is launched\n",
    "- The __grid__ consists of multiple blocks of threads\n",
    "- CUDA kernels use `out` array and don't require explicit type signature.\n",
    "\n",
    "__References__\n",
    "- [Introduction](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#introduction)\n",
    "- [Programming-model](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model)\n",
    "- [CUDA Environment Variables](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars)\n",
    "- [Atomic operations](https://numba.pydata.org/numba-doc/dev/cuda/intrinsics.html#supported-atomic-operations)\n",
    "\n",
    "__Hardware__\n",
    "- [GEFORCE GTX 1050 specs](https://www.nvidia.com/en-in/geforce/products/10series/geforce-gtx-1050/)\n",
    "- [Compute capabilities](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities)\n",
    "- [Hardware Implementation](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation)\n",
    "\n",
    "\n",
    "- In Pascal, an SM (streaming multiprocessor) consists of 128 CUDA cores.\n",
    "- Blocks are mapped to multiprocessors. The multiprocessor creates, manages, schedules, and executes threads in groups of 32 parallel threads called __warps__.\n",
    "\n",
    "__GEFORCE GTX 1050 specs__\n",
    "\n",
    "Parameter | Value\n",
    "--- | ---\n",
    "NVIDIA CUDAÂ® Cores | 640\n",
    "Memory Speed | 7 Gbps\n",
    "Memory Interface Width | 128-bit\n",
    "Memory Bandwidth (GB/sec) | 112"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from numba import cuda, types\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def add_kernel(x, y, out):\n",
    "    idx = cuda.grid(1) # unique thread index within the entire grid\n",
    "    # grid(1) one dimensional thread grid, returns a single value\n",
    "    # same as: idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    out[idx] = x[idx] + y[idx]\n",
    "\n",
    "n = 1280\n",
    "x = np.ones(n).astype(np.int32)\n",
    "y = np.ones_like(x)\n",
    "\n",
    "x_dev = cuda.to_device(x)\n",
    "y_dev = cuda.to_device(y)\n",
    "d_out = cuda.device_array_like(x_dev)\n",
    "\n",
    "# execution configuration\n",
    "# if blocks_per_grid * threads_per_block < n, some entries of the output will be empty\n",
    "blocks_per_grid = 10 # multiplicative of number of SMs\n",
    "# more blocks -> high CUDA kernel launch overhead\n",
    "threads_per_block = 128 # multiplicative of 32\n",
    "\n",
    "add_kernel[blocks_per_grid, threads_per_block](x_dev, y_dev, d_out)\n",
    "print(d_out.copy_to_host()) # implicit cuda.synchronize()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grid Stride Loops\n",
    "When dealing with large datasets and possibly trying to use large grid sizes (that'll result in a high launch overhead), it's possible to use grid stride loops as below. For more details, check [NVIDIA Developer Blog](https://devblogs.nvidia.com/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/)\n",
    "\n",
    "more elements than threads"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def add_kernel(x, y, out):\n",
    "    idx_in_grid = cuda.grid(1) # (1) one dimensional thread grid, returns a single value\n",
    "    stride = cuda.gridsize(1) # same as: cuda.blockDim.x * cuda.gridDim.x\n",
    "    for idx in range(idx_in_grid, x.shape[0], stride):\n",
    "        out[idx] = x[idx] + y[idx]\n",
    "        print(idx) # shown only when run in the terminal with `@cuda.jit(debug=True)`\n",
    "\n",
    "n = 12800\n",
    "x = np.ones(n).astype(np.int32)\n",
    "y = np.ones_like(x)\n",
    "\n",
    "x_dev = cuda.to_device(x)\n",
    "y_dev = cuda.to_device(y)\n",
    "d_out = cuda.device_array_like(x_dev)\n",
    "\n",
    "blocks_per_grid = 10\n",
    "threads_per_block = 128\n",
    "\n",
    "add_kernel[blocks_per_grid, threads_per_block](x_dev, y_dev, d_out)\n",
    "print(d_out.copy_to_host())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working with 2-dimensional datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def add_kernel(x, y, out):\n",
    "    idy,idx = cuda.grid(2)\n",
    "    # grid(2) two dimensional thread grid, returns two values\n",
    "    # stride_y, stride_x = cuda.gridsize(2)\n",
    "\n",
    "    # out[idy,idx] = x[idy,idx] + y[idy,idx]\n",
    "    out[idy,idx] = idy\n",
    "\n",
    "n = 4\n",
    "x = 2 * np.ones((n,n)).astype(np.int32)\n",
    "y = np.ones_like(x)\n",
    "\n",
    "x_dev = cuda.to_device(x)\n",
    "y_dev = cuda.to_device(y)\n",
    "d_out = cuda.device_array_like(x_dev)\n",
    "\n",
    "blocks_per_grid = (2,2)\n",
    "threads_per_block = (2,2)\n",
    "\n",
    "add_kernel[blocks_per_grid, threads_per_block](x_dev, y_dev, d_out)\n",
    "print(d_out.copy_to_host())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Shared memory\n",
    "Shared memory is a programmer defined cache of limited size & it's shared between all threads in a block.\n",
    "\n",
    "- Shared memory is stored in what is called __banks__\n",
    "- [Memory Hierarchy](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def fun_shared(x, y):\n",
    "    idx = cuda.grid(1)\n",
    "    shared_mem = cuda.shared.array(2, dtype=types.int32) # 2 elements in shared memory\n",
    "\n",
    "    shared_mem[idx] = x[idx] # from global to shared memory\n",
    "    cuda.syncthreads() # sync all threads in a block\n",
    "    y[idx] = shared_mem[idx] * 2 # shared to global\n",
    "\n",
    "x = np.arange(3,5).astype(np.int32)\n",
    "y = np.zeros_like(x)\n",
    "\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "\n",
    "%timeit fun_shared[1, 4](d_x, d_y); cuda.synchronize()\n",
    "print(d_y.copy_to_host())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Debugging\n",
    "- Single threaded scenario\n",
    "```\n",
    "%time cuda_fun[1, 1](a,b,out); cuda.synchronize()\n",
    "%timeit cuda_fun[1, 1](a,b,out); cuda.synchronize()\n",
    "```\n",
    "- Use print inside a kernel with `@cuda.jit(debug=True)`\n",
    "- [Numba's CUDA Simulator](https://numba.pydata.org/numba-doc/dev/cuda/simulator.html). Inside a kernel include something similar to\n",
    "```\n",
    "if idy == 1 and idx == 3:\n",
    "        from pdb import set_trace; set_trace(\n",
    "```\n",
    "then run\n",
    "```\n",
    "NUMBA_ENABLE_CUDASIM=1 python cuda_fun.py\n",
    "```\n",
    "[Python Debugger](https://docs.python.org/3/library/pdb.html) commands\n",
    "```\n",
    "n(ext)\n",
    "p expression: Evaluate the expression in the current context and print its value.\n",
    "q(uit)\n",
    "```\n",
    "- Memory issues such as accessing invalid memory addresses may be checked using [Cuda Memcheck](http://docs.nvidia.com/cuda/cuda-memcheck/)\n",
    "```\n",
    "cuda-memcheck python cuda_fun.py\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}